#!/usr/bin/env python3
"""
AIçƒ­ç‚¹æ–°é—»ä¸è®ºæ–‡èšåˆå·¥å…·
ä»å¤šä¸ªæ•°æ®æºè·å–AIç›¸å…³å†…å®¹å¹¶ç”ŸæˆèšåˆæŠ¥å‘Š
"""

import argparse
import json
import os
import re
import sys
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from urllib.parse import urljoin, urlparse

import feedparser
import requests
from bs4 import BeautifulSoup
from openai import OpenAI

# ====================== é…ç½® ======================
DEFAULT_MODEL = "doubao-seed-1-8-251228"
DEFAULT_OUTPUT = "ai_daily_news.json"
DEFAULT_HN_LIMIT = 8  # é»˜è®¤è·å–8æ¡æ–°é—»
DEFAULT_ARXIV_LIMIT = 2  # é»˜è®¤è·å–2ç¯‡è®ºæ–‡
DEFAULT_IMAGE_OUTPUT_DIR = "./images"

# é¢„è®¾çš„å›½å†…æ–°é—»æºï¼ˆRSS feedï¼‰
PREDEFINED_SOURCES = {
    "36kr-ai": {
        "name": "36æ°ªAI",
        "url": "https://36kr.com/information/AI/",
        "type": "html"  # éœ€è¦è§£æHTMLé¡µé¢
    },
    "sina-tech": {
        "name": "æ–°æµªç§‘æŠ€",
        "url": "https://tech.sina.com.cn/",
        "type": "html"
    },
    "zhihu-daily": {
        "name": "çŸ¥ä¹æ—¥æŠ¥",
        "url": "https://daily.zhihu.com/",
        "type": "html"
    }
}

# ====================== Hacker News API ======================
def fetch_hacker_news(limit: int = 10, days_back: int = 1) -> List[Dict]:
    """ä»Hacker Newsè·å–AIç›¸å…³æ–°é—»"""
    base_url = "https://hn.algolia.com/api/v1/search"
    
    # ä½¿ç”¨ç®€å•æŸ¥è¯¢å…³é”®è¯
    query = "AI"
    
    try:
        # è·å–æ•°æ®ï¼ŒæŒ‰pointsæ’åºè·å–çƒ­é—¨å†…å®¹
        response = requests.get(base_url, params={
            "query": query,
            "tags": "story",
            "hitsPerPage": limit * 20  # è·å–æ›´å¤šæ•°æ®ä»¥ä¾¿ç­›é€‰
        }, timeout=10)
        
        response.raise_for_status()
        data = response.json()
        
        hits = data.get("hits", [])
        
        # è®¡ç®—æ—¶é—´è¿‡æ»¤é˜ˆå€¼ï¼ˆæœ€è¿‘6ä¸ªæœˆï¼‰
        cutoff_date = datetime.utcnow() - timedelta(days=180)
        
        # æŒ‰pointsæ’åºåè¿‡æ»¤
        hits_sorted = sorted(hits, key=lambda x: x.get("points", 0), reverse=True)
        
        # è¿‡æ»¤å’Œè½¬æ¢ç»“æœ
        entries = []
        seen_urls = set()
        
        for hit in hits_sorted:
            url = hit.get("url", f"https://news.ycombinator.com/item?id={hit.get('objectID')}")
            
            # å»é‡
            if url in seen_urls:
                continue
            seen_urls.add(url)
            
            # è§£ææ—¥æœŸå¹¶è¿›è¡Œæ—¶é—´è¿‡æ»¤
            created_timestamp = hit.get("created_at_i", 0)
            if created_timestamp:
                created_at = datetime.fromtimestamp(created_timestamp)
                # åªä¿ç•™æœ€è¿‘6ä¸ªæœˆå†…çš„æ–°é—»
                if created_at < cutoff_date:
                    continue
            else:
                created_at = datetime.utcnow()  # fallback
            
            # æå–å›¾ç‰‡ï¼ˆä»OGæ ‡ç­¾ï¼‰
            image = None
            try:
                page_resp = requests.get(url, timeout=5, allow_redirects=True)
                if page_resp.status_code == 200:
                    soup = BeautifulSoup(page_resp.text, 'html.parser')
                    og_image = soup.find('meta', property='og:image')
                    if og_image and og_image.get('content'):
                        image = og_image['content']
            except:
                pass
            
            entries.append({
                "title": hit.get("title", "No title"),
                "url": url,
                "summary": hit.get("points", 0),
                "points": hit.get("points", 0),
                "created_at": created_at.strftime("%Y-%m-%d %H:%M:%S"),
                "source": "hacker_news",
                "type": "news",
                "image": image
            })
            
            if len(entries) >= limit:
                break
        
        print(f"âœ“ ä» Hacker News è·å– {len(entries)} æ¡æ–°é—»")
        return entries
    
    except Exception as e:
        print(f"âœ— Hacker News è·å–å¤±è´¥: {e}")
        return []


# ====================== ArXiv API ======================
def fetch_arxiv_papers(query: str = "artificial intelligence OR machine learning", 
                       limit: int = 10,
                       max_results: int = 20,
                       target_date: Optional[str] = None) -> List[Dict]:
    """ä»ArXivè·å–æœ€æ–°è®ºæ–‡ï¼ˆé»˜è®¤æœ€è¿‘ä¸€ä¸ªæœˆï¼‰"""
    base_url = "http://export.arxiv.org/api/query"
    
    # ArXiv APIå‚æ•°
    params = {
        "search_query": f"all:{query}",
        "start": 0,
        "max_results": max_results,
        "sortBy": "submittedDate",
        "sortOrder": "descending"
    }
    
    try:
        response = requests.get(base_url, params=params, timeout=15)
        response.raise_for_status()
        
        feed = feedparser.parse(response.content)
        
        entries = []
        seen_ids = set()
        
        # è®¡ç®—ä¸€ä¸ªæœˆå‰çš„æ—¥æœŸ
        one_month_ago = datetime.utcnow() - timedelta(days=30)
        
        for entry in feed.entries[:max_results]:
            # æå–ArXiv ID
            arxiv_id = entry.get("id", "").split("/abs/")[-1].split("/v")[0]
            if arxiv_id in seen_ids:
                continue
            seen_ids.add(arxiv_id)
            
            # è§£ææ—¥æœŸ
            published = entry.get("published")
            try:
                if published:
                    pub_date = datetime.strptime(published, "%Y-%m-%dT%H:%M:%SZ")
                else:
                    pub_date = datetime.utcnow()
            except:
                pub_date = datetime.utcnow()
            
            # æ—¥æœŸè¿‡æ»¤
            if target_date:
                try:
                    target = datetime.strptime(target_date, "%Y-%m-%d")
                    # åªè·å–å½“å¤©çš„è®ºæ–‡
                    if pub_date.date() != target.date():
                        continue
                except ValueError:
                    print(f"âš  æ— æ•ˆçš„ç›®æ ‡æ—¥æœŸæ ¼å¼: {target_date}")
                    return []
            else:
                # é»˜è®¤åªè·å–æœ€è¿‘ä¸€ä¸ªæœˆçš„è®ºæ–‡
                if pub_date < one_month_ago:
                    continue
            
            # æå–æ‘˜è¦
            summary = entry.get("summary", "").replace("\n", " ").strip()
            
            # æå–ä½œè€…
            authors = ", ".join([author.name for author in entry.get("authors", [])])
            
            # æŠ½å–å›¾ç‰‡ï¼ˆä»HTMLé¡µé¢ï¼‰
            images = extract_arxiv_images(arxiv_id)
            
            entries.append({
                "title": entry.get("title", ""),
                "url": f"https://arxiv.org/abs/{arxiv_id}",
                "arxiv_id": arxiv_id,
                "summary": summary[:300] + "..." if len(summary) > 300 else summary,
                "authors": authors,
                "published": pub_date.strftime("%Y-%m-%d"),
                "source": "arxiv",
                "type": "paper",
                "images": images  # æ”¯æŒå¤šå¼ å›¾ç‰‡
            })
            
            if len(entries) >= limit:
                break
        
        print(f"âœ“ ä» ArXiv è·å– {len(entries)} ç¯‡è®ºæ–‡")
        return entries
    
    except Exception as e:
        print(f"âœ— ArXiv è·å–å¤±è´¥: {e}")
        return []


def extract_arxiv_images(arxiv_id: str) -> List[str]:
    """ä»ArXivè®ºæ–‡HTMLé¡µé¢æå–æ¶æ„å›¾å’Œæµç¨‹å›¾"""
    try:
        # è®¿é—®ArXiv HTMLç‰ˆæœ¬ï¼ˆæ³¨æ„ï¼šæ·»åŠ trailing slashä»¥ç¡®ä¿urljoinæ­£ç¡®å·¥ä½œï¼‰
        html_url = f"https://arxiv.org/html/{arxiv_id}/"
        response = requests.get(html_url, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡
        images = []
        for img in soup.find_all('img'):
            src = img.get('src', '')
            
            # è¿‡æ»¤ base64 æ•°æ®URL
            if src.startswith('data:'):
                continue
            
            # æ„å»ºå®Œæ•´URL
            if src.startswith('http'):
                image_url = src
            elif src.startswith('//'):
                image_url = f"https:{src}"
            else:
                # ç›¸å¯¹è·¯å¾„ï¼Œä½¿ç”¨urljoinå¤„ç†
                image_url = urljoin(html_url, src)
            
            # è¿‡æ»¤æ‰å…¬å¼å›¾ï¼ˆé€šå¸¸åŒ…å« formulaã€inline ç­‰å…³é”®è¯ï¼‰
            if any(keyword in src.lower() for keyword in ['formula', 'inline', 'math', 'tex', 'equation']):
                continue
            
            # è¿‡æ»¤æ‰å°å›¾æ ‡ï¼ˆé€šå¸¸æ–‡ä»¶ååŒ…å« iconï¼‰
            if 'icon' in src.lower():
                continue
            
            # è·å–å›¾ç‰‡å°ºå¯¸
            width = int(img.get('width', 0)) or int(img.get('data-width', 0)) or 0
            height = int(img.get('height', 0)) or int(img.get('data-height', 0)) or 0
            
            # åªä¿ç•™è¾ƒå¤§çš„å›¾ç‰‡ï¼ˆå¯èƒ½æ˜¯æ¶æ„å›¾æˆ–æµç¨‹å›¾ï¼‰
            if width > 0 and height > 0:
                if width < 200 or height < 200:
                    continue
            
            images.append(image_url)
        
        # æœ€å¤šè¿”å›5å¼ å›¾ç‰‡
        return list(set(images))[:5]  # å»é‡åæœ€å¤š5å¼ 
    
    except Exception as e:
        print(f"  âš  æ— æ³•æå– {arxiv_id} çš„å›¾ç‰‡: {e}")
        return []


# ====================== è‡ªå®šä¹‰æ–°é—»æº ======================
def fetch_custom_news_source(custom_url: str, 
                             source_name: str = "custom",
                             limit: int = 10) -> List[Dict]:
    """ä»è‡ªå®šä¹‰URLè·å–æ–°é—»ï¼ˆæ”¯æŒJSON APIå’ŒRSS/Atom feedï¼‰"""
    
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        
        response = requests.get(custom_url, headers=headers, timeout=10)
        response.raise_for_status()
        
        # å°è¯•è§£æä¸ºRSS/Atom feed
        feed = feedparser.parse(response.content)
        if feed.entries:
            entries = []
            for entry in feed.entries[:limit]:
                url = entry.get('link', '')
                title = entry.get('title', '')
                summary = entry.get('summary', entry.get('description', ''))
                
                # å°è¯•æå–å›¾ç‰‡
                image = None
                if hasattr(entry, 'enclosures') and entry.enclosures:
                    for enc in entry.enclosures:
                        if enc.get('type', '').startswith('image/'):
                            image = enc.get('href')
                            break
                
                entries.append({
                    "title": title,
                    "url": url,
                    "summary": summary[:500] if summary else '',
                    "source": source_name,
                    "type": "news",
                    "image": image,
                    "created_at": datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S")
                })
            
            print(f"âœ“ ä»è‡ªå®šä¹‰æº ({source_name}) è·å– {len(entries)} æ¡æ–°é—»")
            return entries
        
        # å°è¯•è§£æä¸ºJSON API
        try:
            data = response.json()
            # å°è¯•ä¸åŒçš„æ•°æ®ç»“æ„
            items = []
            if isinstance(data, dict):
                for key in ['data', 'items', 'results', 'articles']:
                    if key in data:
                        items = data.get(key, [])
                        if not isinstance(items, list):
                            items = []
                        break
            
            entries = []
            for item in items[:limit]:
                # æå–å­—æ®µï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
                title_field = item.get('title') or item.get('name') or item.get('headline') or ''
                title = title_field.strip() if title_field else ''
                
                url = item.get('url') or item.get('link') or item.get('href') or item.get('sourceUrl') or ''
                
                summary_field = item.get('summary') or item.get('description') or item.get('content') or item.get('abstract') or ''
                summary = summary_field[:500] if summary_field else ''
                
                # æå–å‘å¸ƒæ—¶é—´
                published = item.get('published') or item.get('publish_time') or item.get('created_at') or item.get('pubDate') or item.get('date')
                try:
                    if isinstance(published, str):
                        # å°è¯•å¤šç§æ—¶é—´æ ¼å¼
                        for fmt in ['%Y-%m-%dT%H:%M:%S.%f%z', '%Y-%m-%dT%H:%M:%S%z', '%Y-%m-%d', '%a, %d %b %Y %H:%M:%S %z']:
                            try:
                                pub_time = datetime.strptime(published[:30], fmt)
                                break
                            except:
                                continue
                    else:
                        pub_time = published
                except:
                    pub_time = datetime.utcnow()
                
                # å°è¯•æå–å›¾ç‰‡
                image = item.get('image') or item.get('imageUrl') or item.get('thumbnail') or item.get('imgUrl') or ''
                
                entries.append({
                    "title": title,
                    "url": url,
                    "summary": summary,
                    "source": source_name,
                    "type": "news",
                    "image": image,
                    "created_at": pub_time.strftime("%Y-%m-%d %H:%M:%S") if pub_time else datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S")
                })
            
            print(f"âœ“ ä»è‡ªå®šä¹‰æº ({source_name}) è·å– {len(entries)} æ¡æ–°é—»")
            return entries
        
        except json.JSONDecodeError:
            print(f"âœ— è‡ªå®šä¹‰æº ({source_name}) æ— æ³•è§£æ: æ—¢ä¸æ˜¯æœ‰æ•ˆçš„RSS/Atom feedä¹Ÿä¸æ˜¯JSON API")
            return []
    
    except Exception as e:
        print(f"âœ— è‡ªå®šä¹‰æº ({source_name}) è·å–å¤±è´¥: {e}")
        return []


# ====================== å›½å†…æ–°é—»æºè§£æ ======================
def fetch_domestic_news(source_key: str, limit: int = 10) -> List[Dict]:
    """ä»é¢„è®¾çš„å›½å†…æ–°é—»æºè·å–AIç›¸å…³æ–°é—»"""
    
    if source_key not in PREDEFINED_SOURCES:
        print(f"âœ— æœªçŸ¥çš„å›½å†…æ–°é—»æº: {source_key}")
        return []
    
    source = PREDEFINED_SOURCES[source_key]
    url = source["url"]
    source_name = source["name"]
    
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        entries = []
        
        # AIå…³é”®è¯è¿‡æ»¤
        ai_keywords = ['AI', 'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'LLM', 'å¤§æ¨¡å‹', 'æ™ºèƒ½', 'ChatGPT', 'GPT']
        
        # é€šç”¨è§£æï¼šæŸ¥æ‰¾æ‰€æœ‰å¸¦æ ‡é¢˜å’Œé“¾æ¥çš„å…ƒç´ 
        seen_urls = set()
        link_count = 0
        
        for link in soup.find_all('a', href=True):
            if link_count >= limit * 3:  # å¤šè·å–ä¸€äº›ä»¥ä¾¿ç­›é€‰
                break
            
            href = link.get('href', '')
            title = link.get_text(strip=True)
            
            # è¿‡æ»¤æ‰å¤ªçŸ­çš„æ ‡é¢˜å’Œä¸æ˜¯æ–°é—»çš„é“¾æ¥
            if len(title) < 10 or len(title) > 100 or not href:
                continue
            
            # è¿‡æ»¤æ‰å¯¼èˆªã€é¡µè„šç­‰é“¾æ¥
            skip_keywords = ['ç™»å½•', 'æ³¨å†Œ', 'é¦–é¡µ', 'å…³äº', 'è”ç³»', 'å‹æƒ…é“¾æ¥', 'å¹¿å‘Š', 'åˆä½œ', 'æ‹›è˜']
            if any(keyword in title for keyword in skip_keywords):
                continue
            
            # åªä¿ç•™åŒ…å«AIç›¸å…³å…³é”®è¯çš„æ ‡é¢˜
            if not any(keyword in title for keyword in ai_keywords):
                continue
            
            # å»é‡
            if href in seen_urls:
                continue
            seen_urls.add(href)
            
            if not href.startswith('http'):
                href = urljoin(url, href)
            
            entries.append({
                "title": title,
                "url": href,
                "summary": '',
                "source": source_name,
                "type": "news",
                "image": '',
                "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            })
            
            link_count += 1
            
            if len(entries) >= limit:
                break
        
        print(f"âœ“ ä»å›½å†…æº ({source_name}) è·å– {len(entries)} æ¡æ–°é—»")
        return entries
    
    except Exception as e:
        print(f"âœ— å›½å†…æº ({source_name}) è·å–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return []


# ====================== æ•°æ®æ•´åˆ ======================
def merge_and_sort_entries(hn_entries: List[Dict], 
                          arxiv_entries: List[Dict],
                          custom_entries: List[Dict] = None,
                          domestic_entries: List[Dict] = None) -> List[Dict]:
    """åˆå¹¶å¹¶æ’åºæ‰€æœ‰æ¡ç›®"""
    
    all_entries = hn_entries + arxiv_entries
    
    # æ·»åŠ è‡ªå®šä¹‰æºæ•°æ®
    if custom_entries:
        all_entries += custom_entries
    
    # æ·»åŠ å›½å†…æ–°é—»æºæ•°æ®
    if domestic_entries:
        all_entries += domestic_entries
    
    # æŒ‰æ—¶é—´æ’åº
    sorted_entries = sorted(all_entries, key=lambda x: x.get('created_at', ''), reverse=True)
    
    # å»é‡ï¼ˆåŸºäºURLï¼‰
    seen = set()
    unique_entries = []
    for entry in sorted_entries:
        url = entry.get('url', '')
        if url not in seen:
            seen.add(url)
            unique_entries.append(entry)
    
    return unique_entries


# ====================== æ–‡ç« ç”Ÿæˆ ======================
def generate_article(entries: List[Dict], 
                    model: str = DEFAULT_MODEL,
                    base_url: Optional[str] = None,
                    date: Optional[str] = None) -> str:
    """ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆæ·±åº¦æ–‡ç« """
    
    if not entries:
        return "ä»Šå¤©æ²¡æœ‰æ‰¾åˆ°AIç›¸å…³çš„çƒ­ç‚¹å†…å®¹ã€‚"
    
    # æŒ‰ç±»å‹åˆ†ç»„
    news_items = [e for e in entries if e.get('type') == 'news']
    papers = [e for e in entries if e.get('type') == 'paper']
    
    # å‡†å¤‡æç¤ºè¯
    date_str = date if date else datetime.now().strftime("%Y-%m-%d")
    
    prompt = f"""è¯·åŸºäºä»¥ä¸‹{date_str}çš„AIçƒ­ç‚¹æ–°é—»å’Œæœ€æ–°è®ºæ–‡ï¼Œç”Ÿæˆä¸€ç¯‡æ·±åº¦æ–‡ç« ã€‚

## æ–°é—»çƒ­ç‚¹
{chr(10).join([f"- {n['title']}: {str(n['summary'])[:100]}... (æ¥æº: {n['source']})" for n in news_items])}

## æœ€æ–°è®ºæ–‡
{chr(10).join([f"- {p['title']}: {str(p['summary'])[:100]}... (ä½œè€…: {p['authors']})" for p in papers])}

è¦æ±‚ï¼š
1. ä»¥å¼•äººå…¥èƒœçš„å¼€å¤´å¼•å…¥ä¸»é¢˜
2. å¯¹æ–°é—»çƒ­ç‚¹è¿›è¡Œæ·±å…¥åˆ†æå’Œè¯„è®º
3. é€‰å–1-2ç¯‡é‡è¦è®ºæ–‡è¿›è¡Œè¯¦ç»†ä»‹ç»
4. æä¾›å¯¹AIè¡Œä¸šå‘å±•è¶‹åŠ¿çš„æ´å¯Ÿ
5. ä½¿ç”¨Markdownæ ¼å¼ï¼Œé€‚å½“ä½¿ç”¨emojiå¢åŠ å¯è¯»æ€§
6. æ–‡ç« é•¿åº¦æ§åˆ¶åœ¨800-1200å­—
7. æ¯æ®µå¼€å¤´æ ‡è®°å¼•ç”¨æ¥æºï¼ˆå¦‚[Hacker News], [ArXiv]ï¼‰

ç”Ÿæˆæ–‡ç« :"""

    try:
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        client_kwargs = {"api_key": "dummy"}
        if base_url:
            client_kwargs["base_url"] = base_url
        
        client = OpenAI(**client_kwargs)
        
        # è°ƒç”¨æ¨¡å‹
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIé¢†åŸŸè¯„è®ºå‘˜ï¼Œæ“…é•¿åˆ†ææŠ€æœ¯è¶‹åŠ¿å’Œæ’°å†™æ·±åº¦æ–‡ç« ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=2000
        )
        
        article = response.choices[0].message.content
        
        print(f"âœ“ æ–‡ç« ç”ŸæˆæˆåŠŸ ({len(article)} å­—ç¬¦)")
        return article
    
    except Exception as e:
        print(f"âœ— æ–‡ç« ç”Ÿæˆå¤±è´¥: {e}")
        return f"æ–‡ç« ç”Ÿæˆé‡åˆ°é”™è¯¯ï¼Œä»¥ä¸‹æ˜¯åŸå§‹æ•°æ®ï¼š\n\n{json.dumps(entries, indent=2, ensure_ascii=False)}"


# ====================== ä¸»ç¨‹åº ======================
def main():
    parser = argparse.ArgumentParser(description="AIçƒ­ç‚¹æ–°é—»ä¸è®ºæ–‡èšåˆå·¥å…·")
    
    # æ•°æ®æºé€‰é¡¹
    parser.add_argument("--date", type=str, default=None, 
                      help="æŒ‡å®šç›®æ ‡æ—¥æœŸ (YYYY-MM-DD)ï¼Œä»…ç”¨äºArXivæ—¥æœŸè¿‡æ»¤")
    parser.add_argument("--custom-url", type=str, default=None,
                      help="è‡ªå®šä¹‰æ–°é—»æºURL (æ”¯æŒJSON APIå’ŒRSS/Atom feed)")
    parser.add_argument("--custom-source", type=str, default="custom",
                      help="è‡ªå®šä¹‰æ–°é—»æºåç§° (é»˜è®¤: custom)")
    parser.add_argument("--domestic-source", type=str, default=None,
                      help=f"ä½¿ç”¨é¢„è®¾çš„å›½å†…æ–°é—»æº: {', '.join(PREDEFINED_SOURCES.keys())}")
    parser.add_argument("--domestic-limit", type=int, default=5,
                      help=f"å›½å†…æ–°é—»æºè·å–æ•°é‡ (é»˜è®¤: 5)")
    parser.add_argument("--no-hacker-news", action="store_true",
                      help="ä¸è·å–Hacker Newsæ•°æ®")
    parser.add_argument("--no-arxiv", action="store_true",
                      help="ä¸è·å–ArXivæ•°æ®")
    
    # è¾“å‡ºé€‰é¡¹
    parser.add_argument("--output", "-o", type=str, default=DEFAULT_OUTPUT,
                       help=f"è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„ (é»˜è®¤: {DEFAULT_OUTPUT})")
    parser.add_argument("--image-output-dir", type=str, default=DEFAULT_IMAGE_OUTPUT_DIR,
                       help=f"å›¾ç‰‡ä¿å­˜ç›®å½• (é»˜è®¤: {DEFAULT_IMAGE_OUTPUT_DIR})")
    
    # æ•°é‡é€‰é¡¹
    parser.add_argument("--hn-limit", type=int, default=DEFAULT_HN_LIMIT,
                       help=f"Hacker Newsè·å–æ•°é‡ (é»˜è®¤: {DEFAULT_HN_LIMIT})")
    parser.add_argument("--arxiv-limit", type=int, default=DEFAULT_ARXIV_LIMIT,
                       help=f"ArXivè®ºæ–‡è·å–æ•°é‡ (é»˜è®¤: {DEFAULT_ARXIV_LIMIT})")
    parser.add_argument("--custom-limit", type=int, default=5,
                       help=f"è‡ªå®šä¹‰æºè·å–æ•°é‡ (é»˜è®¤: 5)")
    
    # æ¨¡å‹é€‰é¡¹
    parser.add_argument("--model", type=str, default=DEFAULT_MODEL,
                       help=f"ä½¿ç”¨çš„æ¨¡å‹ (é»˜è®¤: {DEFAULT_MODEL})")
    parser.add_argument("--base-url", type=str, default=None,
                       help="æ¨¡å‹APIçš„base_url")
    parser.add_argument("--no-article", action="store_true",
                       help="ä¸ç”Ÿæˆæ–‡ç« ï¼Œä»…è¾“å‡ºJSONæ•°æ®")
    
    # å…¶ä»–é€‰é¡¹
    parser.add_argument("--days-back", type=int, default=1,
                       help=f"å›æº¯å¤©æ•° (é»˜è®¤: 1)")
    parser.add_argument("--download-images", action="store_true",
                       help="ä¸‹è½½æ‰€æœ‰å›¾ç‰‡åˆ°æœ¬åœ°ç›®å½•")
    
    args = parser.parse_args()
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    print("=" * 60)
    print("AIçƒ­ç‚¹æ–°é—»èšåˆå·¥å…·")
    print("=" * 60)
    print(f"ğŸ“… ç›®æ ‡æ—¥æœŸ: {args.date or 'æœ€æ–°'}")
    print(f"ğŸ¤– æ¨¡å‹: {args.model}")
    if args.base_url:
        print(f"ğŸŒ Base URL: {args.base_url}")
    if args.custom_url:
        print(f"ğŸ”— è‡ªå®šä¹‰æ–°é—»æº: {args.custom_url} ({args.custom_source})")
    if args.domestic_source:
        print(f"ğŸ‡¨ğŸ‡³ å›½å†…æ–°é—»æº: {args.domestic_source}")
    print(f"ğŸ“Š Hacker News: {args.hn_limit if not args.no_hacker_news else 0} æ¡")
    print(f"ğŸ“š ArXiv: {args.arxiv_limit if not args.no_arxiv else 0} æ¡")
    if args.custom_url:
        print(f"ğŸ“° è‡ªå®šä¹‰æº: {args.custom_limit} æ¡")
    if args.domestic_source:
        print(f"ğŸ“° å›½å†…æº: {args.domestic_limit} æ¡")
    print("=" * 60)
    
    # è·å–æ•°æ®
    hn_entries = []
    if not args.no_hacker_news:
        hn_entries = fetch_hacker_news(limit=args.hn_limit, days_back=args.days_back)
    
    arxiv_entries = []
    if not args.no_arxiv:
        arxiv_entries = fetch_arxiv_papers(limit=args.arxiv_limit, max_results=20, target_date=args.date)
    
    # è·å–è‡ªå®šä¹‰æºæ•°æ®
    custom_entries = []
    if args.custom_url:
        custom_entries = fetch_custom_news_source(
            args.custom_url,
            source_name=args.custom_source,
            limit=args.custom_limit
        )
    
    # è·å–å›½å†…æ–°é—»æºæ•°æ®
    domestic_entries = []
    if args.domestic_source:
        domestic_entries = fetch_domestic_news(
            args.domestic_source,
            limit=args.domestic_limit
        )
    
    # æ•´åˆæ•°æ®
    all_entries = merge_and_sort_entries(hn_entries, arxiv_entries, custom_entries, domestic_entries)
    
    # ä¸‹è½½å›¾ç‰‡
    if args.download_images:
        os.makedirs(args.image_output_dir, exist_ok=True)
        
        image_count = 0
        for entry in all_entries:
            # å¤„ç†å•å¼ å›¾ç‰‡ (Hacker News)
            image_url = entry.get('image')
            if image_url:
                try:
                    img_resp = requests.get(image_url, timeout=5)
                    if img_resp.status_code == 200:
                        # ç”Ÿæˆæ–‡ä»¶å
                        ext = os.path.splitext(urlparse(image_url).path)[1] or '.jpg'
                        filename = f"{entry['source']}_{entry['type']}_0{ext}"
                        filepath = os.path.join(args.image_output_dir, filename)
                        
                        with open(filepath, 'wb') as f:
                            f.write(img_resp.content)
                        
                        print(f"  âœ“ ä¸‹è½½å›¾ç‰‡: {filename}")
                        image_count += 1
                except Exception as e:
                    print(f"  âœ— ä¸‹è½½å›¾ç‰‡å¤±è´¥ ({image_url}): {e}")
            
            # å¤„ç†å¤šå¼ å›¾ç‰‡ (ArXiv)
            images = entry.get('images', [])
            for i, image_url in enumerate(images):
                try:
                    img_resp = requests.get(image_url, timeout=5)
                    if img_resp.status_code == 200:
                        # ç”Ÿæˆæ–‡ä»¶å
                        ext = os.path.splitext(urlparse(image_url).path)[1] or '.jpg'
                        filename = f"{entry['source']}_{entry['type']}_0{i+1}{ext}"
                        filepath = os.path.join(args.image_output_dir, filename)
                        
                        with open(filepath, 'wb') as f:
                            f.write(img_resp.content)
                        
                        print(f"  âœ“ ä¸‹è½½å›¾ç‰‡: {filename}")
                        image_count += 1
                except Exception as e:
                    print(f"  âœ— ä¸‹è½½å›¾ç‰‡å¤±è´¥ ({image_url}): {e}")
        
        print(f"âœ“ å…±ä¸‹è½½ {image_count} å¼ å›¾ç‰‡åˆ° {args.image_output_dir}/")
    
    # è¾“å‡ºJSON
    with open(args.output, 'w', encoding='utf-8') as f:
        json.dump({
            "date": args.date or datetime.now().strftime("%Y-%m-%d"),
            "generated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "entries": all_entries,
            "stats": {
                "total": len(all_entries),
                "news": len([e for e in all_entries if e.get('type') == 'news']),
                "papers": len([e for e in all_entries if e.get('type') == 'paper'])
            }
        }, f, indent=2, ensure_ascii=False)
    
    print(f"âœ“ æ•°æ®å·²ä¿å­˜åˆ° {args.output}")
    print("=" * 60)
    
    # ç”Ÿæˆæ–‡ç« 
    if not args.no_article:
        print("\nğŸ¤– æ­£åœ¨ç”Ÿæˆæ–‡ç« ...\n")
        article = generate_article(all_entries, model=args.model, base_url=args.base_url, date=args.date)
        print("\n" + "=" * 60)
        print("ç”Ÿæˆçš„æ–‡ç« :")
        print("=" * 60)
        print(article)
        print("=" * 60)
        
        # ä¿å­˜æ–‡ç« 
        article_file = args.output.replace('.json', '_article.md')
        with open(article_file, 'w', encoding='utf-8') as f:
            f.write(article)
        print(f"âœ“ æ–‡ç« å·²ä¿å­˜åˆ° {article_file}")


if __name__ == "__main__":
    main()
